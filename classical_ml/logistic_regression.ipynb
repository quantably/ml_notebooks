{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "$\n",
    "z = w^Tx = w_0 + w_1x_1 + w_2x_2 + \\dots w_p x_p\\\\\n",
    "g(z) = \\dfrac{1}{1+e^{-z}}\\\\\n",
    "$\n",
    "\n",
    "then our logistic regression model is given by: \n",
    "\n",
    "$\n",
    "h_w(x)= g(w^Tx)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could proceed as for linear regression by using the mean-squared error as our loss function. Unfortunately because of the non-linear nature of the sigmoid function, this loss function is not convex and so there are many local optima. This is a problem for gradient descent algorithm as it may not converge to the global minimum. Cross-entropy loss measures the performance of a classifier that outputs a 0 or 1. For a single training example (x,y) we define this as: \n",
    "\n",
    "$\n",
    "\\mathcal{L}(h_w, y) = \n",
    "     \\begin{cases}\n",
    "       -\\text{log}(h_w(x)) \\quad\\text{if }y=1\\\\\n",
    "       -\\text{log}(1 - h_w(x)) \\quad\\text{if }y=0\\\\\n",
    "     \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these loss functions for y = 0 and y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_w = np.linspace(0.0001, 0.9999, 50)\n",
    "cost_y1 = -np.log(h_w)\n",
    "cost_y0 = -np.log(1-h_w)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True)\n",
    "ax = axs[0]\n",
    "ax.set_title(\"Loss function for y=1\")\n",
    "ax.set_xlabel(r\"$\\hat{y}$\")\n",
    "ax.set_ylabel(r\"$\\mathcal{L}(h_w(x), y)$\")\n",
    "ax.plot(h_w, cost_y1)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"Loss function for y=0\")\n",
    "ax.set_xlabel(r\"$\\hat{y}$\")\n",
    "ax.set_ylabel(r\"$\\mathcal{L}(h_w(x), y)$\")\n",
    "ax.plot(h_w, cost_y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the prediction is close the actual value of y then the loss functions are small. Conversly when the predictions are far from the actual y values then the loss is large. Therefore, we are penalised more when our predictions are further from the truth!\n",
    "\n",
    "It is actually possible to rewrite the loss function above as a single function as follows:\n",
    "\n",
    "$\n",
    "\\mathcal{L}(h_w, y) =-y\\,\\text{log}(h_w(x)) - (1-y)\\text{log}(1 - h_w(x))\n",
    "$\n",
    "\n",
    "The above accounts for the loss for a single training example, we define a cost funtion $\\mathcal{J}$ that is the average loss across all the training examples:\n",
    "\n",
    "$\n",
    "\\mathcal{J}(w) =-\\frac{1}{N}\\sum_{i=0}^{N}\\left[{y^{(i)}\\,\\text{log}(h_w(x^{(i)})) + (1-y^{(i)})\\,\\log(1 - h_w(x^{(i)}))}\\right]\n",
    "$\n",
    "\n",
    "where $x^{(i)}$ represents the $i^{th}$ training example. Our goal then is to find the $w$ values that minimise $\\mathcal{J}$. We will do this using gradient descent approach and for this the first thing we need to compute the deriviative $\\mathcal{J}$ with respect to our weights $w_j$. \n",
    "\n",
    "In the following steps, we will use the notation $wx^{i}$ to mean $w_0 + w_1x_1^{i} + w_2x_2^{i} + \\dots + w_px_p^{i}$\n",
    "\n",
    "We can simplify $\\mathcal{J}(w)$ by noting that:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\log(h_w(x^{i})) &= \\log \\left( \\dfrac{1}{1+e^{-wx^{i}}}\\right)\\\\\n",
    "&= \\log(1) - \\log(1+e^{-wx^{i}})\\\\\n",
    "&= - \\log(1+e^{-wx^{i}})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\log(1 - h_w(x^{i})) &= \\log \\left( 1 - \\dfrac{1}{1+e^{-wx^{i}}}\\right)\\\\\n",
    "&=\\log\\left(\\dfrac{e^{-wx^{i}}}{1 + e^{-wx^{i}}}\\right)\\\\\n",
    "&=\\log\\left( e^{-wx^{i}} \\right) - \\log\\left( 1 + e^{-wx^{i}} \\right)\\\\\n",
    "&=-wx^{i} - \\log\\left(1 + e^{-wx^{i}}\\right)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Substituting these results back into $\\mathcal{J}(w)$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{J}(w) &= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left[ y^{i}\\log\\left( 1 + e^{-wx^{i}}\\right) + (1-y^{i})\\left( wx^{i} + \\log\\left( 1 + e^{-wx^{i}} \\right)\\right)\\right]\\\\\n",
    "&= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left[ y^{i}\\log\\left( 1 + e^{-wx^{i}}\\right) + wx^{i} + \\log\\left( 1 + e^{-wx^{i}} \\right) -y^{i}wx^{i} -y^{i}\\log\\left( 1 + e^{-wx^{i}} \\right) \\right]\\\\\n",
    "&= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left[ wx^{i} + \\log\\left( 1 + e^{-wx^{i}} \\right) -y^{i}wx^{i} \\right]\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Then by writing $wx^{i}$ as $\\log\\left( e^{wx^{i}}\\right)$ we get: \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{J}(w) &= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left[ \\log\\left( e^{wx^{i}}\\right) + \\log\\left( 1 + e^{-wx^{i}} \\right) -y^{i}wx^{i} \\right]\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "and then using $\\log\\left(a\\right) + \\log\\left(b\\right) = \\log \\left( ab\\right)$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{J}(w) &= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left[ \\log\\left( 1 + e^{wx^{i}} \\right) -y^{i}wx^{i} \\right]\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Then we need to find the deriviatives:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "&\\frac{\\partial}{\\partial w_j }\\log\\left( 1 + e^{wx^{i}} \\right) = \\dfrac{x_j^{i}e^{wx^{i}}}{1 + e^{wx^{i}}} = x_j^{i} h_{w}\\left( x^{\\left(i\\right)}\\right)\\\\\n",
    "&\\frac{\\partial}{\\partial w_j }y^{i}wx^{i} = y^{i}x_{j}^{i}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{J}\\left(w\\right)}{\\partial w_j } &= \\dfrac{1}{N} \\sum_{i=0}^{N} x_j^{i} h_{w}\\left( x^{\\left(i\\right)}\\right) - y^{i}x_{j}^{i} \\\\\n",
    "&= \\dfrac{1}{N} \\sum_{i=0}^{N} \\left(h_{w}( x^{\\left(i\\right)}) - y^{i}\\right)x_j^{i}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "So our weight update rule via gradient descent is then: \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "w_j := w_j - \\dfrac{\\alpha}{N} \\sum_{i=0}^{N} \\left(h_{w}( x^{\\left(i\\right)}) - y^{i}\\right)x_j^{i}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build a python function to compute the cost function at each step of gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\mathcal{J}(w) =-\\frac{1}{N}\\sum_{i=0}^{N}\\left[{y^{(i)}\\,\\text{log}(h_w(x^{(i)})) + (1-y^{(i)})\\,\\log(1 - h_w(x^{(i)}))}\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "w_j := w_j - \\dfrac{\\alpha}{N} \\sum_{i=0}^{N} \\left(h_{w}( x^{\\left(i\\right)}) - y^{i}\\right)x_j^{i}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict(features, weights):\n",
    "    return sigmoid(np.dot(features, weights.T))\n",
    "\n",
    "\n",
    "def compute_cost(features, labels, weights):\n",
    "    n_obs = len(features)\n",
    "    predictions = predict(features, weights)\n",
    "    y1_cost = labels * np.log(predictions)\n",
    "    y0_cost = (1 - labels) * np.log(1 - predictions)\n",
    "    total_cost = -(y1_cost + y0_cost).sum()\n",
    "    return total_cost / n_obs\n",
    "\n",
    "\n",
    "def update_weights(features, labels, weights):\n",
    "    alpha = 0.01\n",
    "    predictions = predict(features, weights)\n",
    "    gradient = np.dot(features.T, predictions - labels)/len(features)\n",
    "    return weights - alpha * gradient\n",
    "\n",
    "\n",
    "def gradient_descent(features, labels):\n",
    "    # pick a random w to start\n",
    "    D = features.shape[1]\n",
    "    w = np.random.randn(D) * 1 / np.sqrt(D)\n",
    "    costs = []\n",
    "\n",
    "    for i in range(2000):\n",
    "        cost = compute_cost(features, labels, w)\n",
    "        w = update_weights(features, labels, w)\n",
    "        costs.append(cost)\n",
    "    return costs, w\n",
    "\n",
    "\n",
    "def create_fake_data():\n",
    "    X, y = make_blobs(n_samples=1000, centers=2, cluster_std=2, random_state=22)\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some fake data that we want to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = create_fake_data()\n",
    "\n",
    "X_train = X[0:500]\n",
    "y_train = y[0:500]\n",
    "\n",
    "X_test = X[500:]\n",
    "y_test = y[500:]\n",
    "\n",
    "plt.scatter(X_train[:,1], X_train[:,2], c=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs, w_learnt = gradient_descent(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize = np.vectorize(decision_boundary)\n",
    "predictions = binarize(predict(X_test, w_learnt)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.flatten(), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points_on_line(x, w):\n",
    "    return (w[0] + (w[1] * x))/-w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(-12,5,50)\n",
    "line = get_points_on_line(x_range, w_learnt)\n",
    "plt.scatter(X_train[:,1], X_train[:,2], c=y_train)\n",
    "plt.plot(x_range, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

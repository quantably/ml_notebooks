{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "For a bayesian linear regression, as oridinary least squares regression, we first assume that our target variable, $t$ is given by a deterministic function:\n",
    "\n",
    "$$\n",
    "y(\\textbf{x}, \\textbf{w}) + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ is a zero mean Gaussian variable with inverse variance $\\beta$. Thus we can write:\n",
    "\n",
    "$$p(t\\vert \\textbf{x}, \\textbf{w}, \\beta) = \\mathcal{N}(t \\vert y(\\textbf{x}, \\textbf{w}), \\beta^{-1})$$\n",
    "\n",
    "which is a conditional Gaussian distribution with mean $y(\\textbf{x}, \\textbf{w})$ and inverse variance $\\beta$.\n",
    "\n",
    "For our prior, if we use a zero-mean isotropic Gaussian governed by a single precision parameter $\\alpha$ so that: \n",
    "\n",
    "$$p(\\textbf{w}\\vert\\alpha) = \\mathcal{N}({\\textbf{w}\\vert \\textbf{0}}, \\alpha^{-1}\\textbf{I})$$\n",
    "\n",
    "then, the corresponding posterior has a closed form solution given by:\n",
    "\n",
    "$$p(\\textbf{w}\\vert\\textbf{t}) = \\mathcal{N}(\\textbf{w}\\vert\\textbf{m}_{N}, \\textbf{S}_{N})$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{m}_{N} &= \\beta \\textbf{S}_{N} \\Phi^{T}\\textbf{t}\\\\\n",
    "\\textbf{S}_N^{-1} &= \\alpha \\textbf{I} + \\beta \\Phi^{T}\\Phi\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In practice, we are not usaully interested in the single values of $w$ but rather in making predictions of $t$ for new values of $\\textbf{x}$. This is done by using the predictive distribution, defined as follows: \n",
    "\n",
    "$$p(t\\vert\\textbf{t},\\alpha,\\beta) = \\int p(t\\vert\\textbf{w},\\beta)p(\\textbf{w}\\vert\\textbf{t},\\alpha,\\beta)$$\n",
    "\n",
    "The conditional distribution $p(t\\vert\\textbf{w}\\beta)$ is defined in the model above. $p(\\textbf{w}\\vert\\textbf{t},\\alpha,\\beta)$ represents our posterior distribution which is given by the closed form solution above. The predictive distribution is the convolution of two Gaussian distributions meaning that the predictive distribution is also Gaussian and is defined as follows: \n",
    "\n",
    "$$p(t\\vert\\textbf{x},\\textbf{t},\\alpha,\\beta) = \\mathcal{N}(t\\vert\\textbf{m}_{N}^{T}\\phi(\\textbf{x}), \\sigma_{N}^{2}(\\textbf{x}))$$\n",
    "\n",
    "where the variance $\\sigma_{N}^{2}$ is given by:\n",
    "\n",
    "$$\\sigma_{N}^{2} = \\dfrac{1}{\\beta} + \\phi(\\textbf{x})^{T}\\textbf{S}_{N}\\phi{\\textbf{(x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from basis_expansion_utils import *\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = -0.3\n",
    "a1 = 0.5\n",
    "\n",
    "def noise(size, variance):\n",
    "    return np.random.normal(scale=np.sqrt(variance), size=size)\n",
    "\n",
    "def f(X, noise_variance):\n",
    "    return a0 + a1 * X + noise(X.shape, noise_variance)\n",
    "\n",
    "def sinusoidal(X, noise_variance):\n",
    "    return np.sin(2 * np.pi * X) + noise(X.shape, noise_variance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(alpha, beta, phi, t):\n",
    "    '''mean and covariance matrix of the posterior'''\n",
    "    S_N_inv = alpha * np.eye(phi.shape[1]) + beta * np.dot(phi.T,phi)\n",
    "    S_N = np.linalg.inv(S_N_inv)\n",
    "    m_N = beta * S_N.dot(phi.T).dot(t)\n",
    "    return m_N, S_N\n",
    "\n",
    "def predictive(phi, m_n, S_n, beta):\n",
    "    mean = m_N.T.dot(phi.T).reshape(-1,1)\n",
    "    # TODO: why doesn't below work!!\n",
    "    #variance = 1/beta + np.sum(phi.dot(S_N).dot(phi.T), axis=1).reshape(-1,1)\n",
    "    var = 1 / beta + np.sum(phi.dot(S_N) * phi, axis=1)\n",
    "    return mean, var\n",
    "\n",
    "def plot_truth(X,y):\n",
    "    plt.plot(X,y,\"--\", c=\"black\", label=\"truth\")\n",
    "\n",
    "def plot_observed(X,y):\n",
    "    plt.scatter(X[:,0:],t, c=\"black\", label=\"observed\")\n",
    "    \n",
    "def plot_posterior_samples(X, y_sample):\n",
    "    for c in range(y_sample.shape[1]):\n",
    "        plt.plot(X.ravel(), y_sample[:,c], c=\"red\")\n",
    "        \n",
    "def plot_predictive(X,y,std):\n",
    "    X = X.ravel()\n",
    "    y = y.ravel()\n",
    "    std = std.ravel()\n",
    "    plt.plot(X,y)\n",
    "    plt.fill_between(X, y + std, y - std, alpha = 0.5)\n",
    "    \n",
    "def plot_posterior(m_N, S_N):\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel(\"$w_{0}$\")\n",
    "    plt.ylabel(\"$w_{1}$\")\n",
    "    w0_vals = np.linspace(-1,1,100)\n",
    "    w1_vals = np.linspace(-1,1,100)\n",
    "    stacked = np.dstack(np.meshgrid(w0_vals, w1_vals))\n",
    "    Z = stats.multivariate_normal.pdf(stacked, mean=m_N.ravel(), cov=S_N)\n",
    "    plt.contour(w0_vals, w1_vals, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic data\n",
    "beta = 25\n",
    "alpha = 2\n",
    "variance = 1/beta\n",
    "\n",
    "X = (np.random.rand(6,1) * 2 - 1)\n",
    "t = f(X, noise_variance=variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = basis_expansion(X)\n",
    "post_mean, post_var = posterior(alpha, beta, phi, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-1,1,100).reshape(-1, 1)\n",
    "phi_test = basis_expansion(X_test)\n",
    "y_true = f(X_test, noise_variance=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becasue the posterior is gaussian, the mode coincides with the mean. Therefore, the maximum posterior weight vector is given by $\\textbf{w}_{\\text{MAP}} = \\textbf{m}_{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,t, \".\")\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1.5, 1)\n",
    "plt.plot(X_test, phi_test.dot(post_mean))\n",
    "plt.plot(X_test, y_true, '--', c=\"black\", label=\"truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 15))\n",
    "plt.subplots_adjust(hspace=0.1, wspace=1)\n",
    "\n",
    "N = [0, 1, 3, 25]\n",
    "for i, n in enumerate(N): \n",
    "    X = (np.random.rand(n,1) * 2 - 1)\n",
    "    t = f(X, noise_variance=variance)\n",
    "    phi = basis_expansion(X, identity_basis)\n",
    "    m_N, S_N = posterior(alpha, beta, phi, t)\n",
    "      \n",
    "    post_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5)\n",
    "    y, y_var = predictive(phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    plt.subplot(len(N), 3, i * 3 + 1)\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.title(f\"Posterior Samples for n={n}\")\n",
    "\n",
    "    plot_truth(X_test,y_true)\n",
    "    plot_observed(X,t)\n",
    "    y_sample = post_samples.dot(phi_test.T).T\n",
    "    plot_posterior_samples(X_test, y_sample)\n",
    "    \n",
    "    plt.subplot(len(N), 3, i * 3 + 2)\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.title(\"Predictive Distribution\")\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plot_observed(X,t)\n",
    "    plot_truth(X_test,y_true)\n",
    "    \n",
    "    # plt posterior\n",
    "    plt.subplot(len(N), 3, i * 3 + 3)\n",
    "    plt.scatter(a0, a1, marker=\"+\", c=\"red\")\n",
    "    plot_posterior(m_N, S_N)\n",
    "    plt.title(\"Posterior Distribution\")\n",
    "\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate some non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot params\n",
    "plt.figure(figsize=(13, 15))\n",
    "plt.subplots_adjust(hspace=0.1, wspace=1)\n",
    "\n",
    "\n",
    "# truth data\n",
    "x_test = np.linspace(0,1,100).reshape(-1, 1)\n",
    "mus = np.linspace(0, 1, 9)\n",
    "phi_test = basis_expansion(x_test, gaussian_radial_basis, mus=mus)\n",
    "t_true = sinusoidal(x_test, noise_variance=0)\n",
    "alpha = 2\n",
    "beta = 25\n",
    "\n",
    "N_sizes = [0,1,3,20,50]\n",
    "\n",
    "for i, n in enumerate(N_sizes):\n",
    "\n",
    "    X = np.random.rand(n, 1)\n",
    "    phi = basis_expansion(X, gaussian_radial_basis, mus=mus)\n",
    "    t = sinusoidal(X, noise_variance=1/beta)\n",
    "\n",
    "    m_N, S_N = posterior(alpha, beta, phi, t)\n",
    "    post_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = phi_test.dot(post_samples)\n",
    "    \n",
    "    plt.subplot(len(N_sizes),2,2*i+1)\n",
    "    plot_posterior_samples(x_test, y_samples)\n",
    "    plot_truth(x_test, t_true)\n",
    "    plot_observed(X, t)\n",
    "    y, y_var = predictive(phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    plt.subplot(len(N_sizes),2, 2*i+2)\n",
    "    plot_predictive(x_test, y, np.sqrt(y_var))\n",
    "    plot_observed(X, t)\n",
    "    plot_truth(x_test, t_true)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

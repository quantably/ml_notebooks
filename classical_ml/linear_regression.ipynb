{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although an analytical formula exists for linear regression. It is still possible to solve for the optimal weights using gradient descent.\n",
    "Recall that weight updates via gradient descent are computed as follows: \n",
    "\n",
    "$\n",
    "w_j := w_j - \\alpha \\dfrac{\\partial}{\\partial w_j}\\text{Loss}(w)\n",
    "$\n",
    "\n",
    "for linear regression our hypothesis is defined as follows: \n",
    "\n",
    "$\n",
    "h_w(x) = w^{T}x = w_0 + w_1 x_1 + \\dots + w_p x_p\n",
    "$\n",
    "\n",
    "where $x_0$ is assumed to be 1 to give us the bias term.\n",
    "\n",
    "For linear regression, we use Mean Squared Error as our loss function. For $N$ training examples, this is defined as follows:\n",
    "\n",
    "$\n",
    "\\mathcal{L}(h_w(x), y) = \\sum_{i=0}^{N} (h_w(x^{(i)}) - y^{(i)})^2\n",
    "$\n",
    "\n",
    "where $x^{(i)}$ is taken to mean the $i^{\\text{th}}$ training example. Our aim is to find the values of $w$ that minimise this loss function. It's trivial to show that:\n",
    "\n",
    "$\n",
    "\\dfrac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{-2}{N} \\sum_{i=0}^{N}(y^{(i)}-w^{T}x^{(i)})x^{(i)}_j\n",
    "$\n",
    "\n",
    "Substituting this into our gradient descent equation, we get the following weight update rule: \n",
    "\n",
    "$\n",
    "w_j := w_j + \\dfrac{\\alpha}{N} \\sum_{i=0}^{N}(y^{(i)}-w^{T}x^{(i)})x^{(i)}_j\n",
    "$\n",
    "\n",
    "Let's now write the python code to perform this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "w = np.random.randn(2)\n",
    "features = np.c_[np.ones(500), np.random.normal(size=500)]\n",
    "y = np.dot(w, features.T) + np.random.normal(size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features[:, 1], y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, features, labels):\n",
    "    alpha = 0.01\n",
    "    \n",
    "    predictions = np.dot(features, w)\n",
    "    residuals = labels - predictions\n",
    "    loss = np.dot(features.T, residuals)\n",
    "    \n",
    "    adjustment = loss.sum()\n",
    "    adjustment = (adjustment * alpha)/len(features)\n",
    "    return w + adjustment\n",
    "\n",
    "def gradient_descent(features, labels):\n",
    "    D = features.shape[1]\n",
    "    w = np.random.randn(D)\n",
    "    \n",
    "    for i in range(5000):\n",
    "        w = update_weights(w, features, labels)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gradient_descent(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.c_[np.ones(50), np.linspace(-4, 4, 50)]\n",
    "ys = np.dot(xs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(features[:, 1], y)\n",
    "plt.plot(xs[:, 1], ys, c='r')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
